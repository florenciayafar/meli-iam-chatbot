# ğŸš€ GUÃA DE EJECUCIÃ“N - MELI IAM CHALLENGE

## ğŸŒŸ **MÃ‰TODO 1**
```
### ğŸš€ **Ejecutar:**

```bash

# 1. Instalar todas las dependencias
./venv/bin/pip install -r requirements.txt

# 2. Configurar base de datos (primera vez)
./venv/bin/python setup_database.py

# 3. Ejecutar ambos servicios
./venv/bin/python start_meli_app.py
```

### ğŸŒ **URLs automÃ¡ticas:**
- **Frontend:** http://localhost:8503
- **API:** http://localhost:8000  
- **DocumentaciÃ³n:** http://localhost:8000/docs

### ğŸ›‘ **Detener:**
```bash
Ctrl+C
```

---

## ğŸ³ **MÃ‰TODO 2: DOCKER (TAMBIÃ‰N FÃCIL)**


### ğŸš€ **Ejecutar:**
```bash
# Construir y ejecutar
docker-compose up --build

# O en background
docker-compose up --build -d
```

### ğŸŒ **URLs disponibles:**
- **Frontend:** http://localhost:8503
- **API:** http://localhost:8000  
- **DocumentaciÃ³n:** http://localhost:8000/docs

### ğŸ›‘ **Detener:**
```bash
# Si estÃ¡ en foreground
Ctrl+C

# Si estÃ¡ en background  
docker-compose down
```


## âš ï¸ **SOLUCIÃ“N DE PROBLEMAS**

### ğŸ”¥ **Error: `./venv/bin/python no existe`**
```bash
# Recrear entorno virtual
rm -rf venv
python3 -m venv venv
./venv/bin/pip install streamlit requests
```

### ğŸ”¥ **Error: `ModuleNotFoundError`**
```bash
# Instalar dependencia especÃ­fica
./venv/bin/pip install [nombre-modulo]

# O todas las dependencias
./venv/bin/pip install -r requirements.txt
```

### ğŸ”¥ **Error: `Port already in use`**
```bash
# Matar procesos en puertos
pkill -f streamlit
pkill -f uvicorn

# O usar otros puertos
./venv/bin/python -m streamlit run src/frontend/meli_app.py --server.port 8504
```

### ğŸ”¥ **Error: `API not responding`**
```bash

./venv/bin/python -m streamlit run src/frontend/meli_app.py --server.port 8503
```

---

# ğŸ“‹ MeLi IAM Challenge - DocumentaciÃ³n TÃ©cnica Completa

## Estructura 

ğŸ“ meli-iam-chatbot/
â”œâ”€â”€ ğŸš€ start_meli_app.py          
â”œâ”€â”€ ğŸ“‹ README.md                   # GuÃ­a rÃ¡pida de uso
â”œâ”€â”€ âš™ï¸  setup_database.py          # ConfiguraciÃ³n inicial BD
â”œâ”€â”€ ğŸ“¦ requirements.txt            # Dependencias Python
â”œâ”€â”€ ğŸ”§ .env                        # Variables de entorno
â”œâ”€â”€ ğŸ“ config.env.template         # Template configuraciÃ³n
â”œâ”€â”€ ğŸš« .gitignore                  # Git ignore
â”‚
â”œâ”€â”€ ğŸ“ src/ (13 archivos Python)    # CÃ“DIGO FUENTE 
â”‚   â”œâ”€â”€ frontend/meli_app.py       # Frontend
â”‚   â”œâ”€â”€ api/main.py + models.py    # FastAPI 
â”‚   â”œâ”€â”€ bot/llm_interface.py       # Interfaz Llama 3
â”‚   â”œâ”€â”€ rag/vector_store.py        # ChromaDB + RAG
â”‚   â”œâ”€â”€ memory/conversation_memory.py # Memoria conversacional
â”‚   â””â”€â”€ utils/document_processor.py   # Procesamiento PDFs
â”‚
â”œâ”€â”€ ğŸ“ data/                       # BASE DE DATOS + DOCUMENTOS
â”‚   â”œâ”€â”€ chroma/                    # Vector database (ChromaDB)
â”‚   â”œâ”€â”€ documents/                 # PDFs + documentaciÃ³n IAM
â”‚   â””â”€â”€ processed/                 # Chunks procesados
â”‚
â””â”€â”€ ğŸ“ venv/                        # ğŸ Entorno virtual Python

## ğŸ¯ Decisiones de DiseÃ±o

### Arquitectura del Sistema
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   FastAPI       â”‚    â”‚   ChromaDB      â”‚
â”‚   Streamlit     â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚   Vector Store  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Llama 3       â”‚
                       â”‚   via Ollama    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. **Â¿Por quÃ© FastAPI?**
- **Performance**: AsÃ­ncrono por defecto, ideal para LLM calls
- **DocumentaciÃ³n automÃ¡tica**: OpenAPI/Swagger integrado
- **Type hints**: ValidaciÃ³n automÃ¡tica de requests
- **Ecosystem**: Compatible con Pydantic, Uvicorn, etc.

### 2. **Â¿Por quÃ© Llama 3?**
- **Open Source**: Cumple requisito de LLM abierto
- **EspecializaciÃ³n**: Excelente para Q&A tÃ©cnico
- **Local**: Sin dependencias de APIs externas
- **Customizable**: Control total sobre prompts

### 3. **Â¿Por quÃ© ChromaDB?**
- **Vector Database**: BÃºsqueda semÃ¡ntica eficiente
- **Embeddings**: sentence-transformers integrado
- **Persistent**: Datos persisten entre reinicios
- **Metadata**: Filtros por documento/categorÃ­a

### 4. **Â¿Por quÃ© memoria custom?**
- **Requisito**: "NO usar librerÃ­as ni LLM que tengan memoria"
- **Control total**: Buffer size, estrategias, persistence
- **Performance**: Optimizado para casos de uso especÃ­ficos

### MÃ©tricas de Performance
- **Tiempo promedio de respuesta**: 15-20 segundos
- **PrecisiÃ³n RAG**: ~70% (mejora pendiente)
- **Memoria conversacional**: 29 sesiones activas
- **Documentos procesados**: 11 archivos IAM (7,000+ lÃ­neas)

## ğŸš€ Resultados TÃ©cnicos

### Arquitectura RAG
- **Embedding Model**: sentence-transformers/all-MiniLM-L6-v2
- **Chunk Strategy**: Texto dividido por pÃ¡rrafos
- **Context Window**: 10 chunks mÃ¡ximo
- **Temperature**: 0.2 (balance precisiÃ³n/creatividad)

### Memory Management
- **Estrategia**: Buffer con lÃ­mite de mensajes
- **Persistencia**: JSON file-based
- **Session IDs**: Ãšnicos por usuario
- **Cleanup**: AutomÃ¡tico en shutdown

### API Endpoints
- **POST /api/chat**: ConversaciÃ³n principal
- **GET /health**: Health check
- **GET /docs**: DocumentaciÃ³n automÃ¡tica

